\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{
    Super Convergence: Very Fast Training of Residual Networks Using Large
    Learning Rates\\
    \large ICLR Reproducibility Challenge}

\author{Keivaun Waugh\\
University of Texas at Austin\\
{\tt\small keivaunwaugh@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Paul Choi\\
University of Texas at Austin\\
{\tt\small choipaul96@gmail.com}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
In this paper, we aim to evaluate the reproducibility of the experiments
    detailed in the paper ``Super-Convergence: Very Fast Training of Residual
    Networks Using Large Learning Rates'' \cite{SuperConvergence}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
In an ICLR 2018 submission, the anonymous authors of \cite{SuperConvergence}
show that under certain training conditions, a phenomenon called
``super-convergence'' can be achieved, where ResNets \cite{ResNet} can be
trained an order of magnitude more quickly than when using normal training
methods.  Super-convergence simply requires using a cyclical learning rate
(CLR) versus the standard piecewise constant learning rate (PCLR). When using
the CLR method, the learning rate is slowly increased for a minimum learning
rate to a maximum learning rate, then slowly decreased back to the minimum
learning rate again, after which training stops. The number of iterations taken
to go from the minimum learning rate to the maximum and vice versa is referred
to as the step-size and is a hyperparameter.

The authors run a variety of experiments to see under what circumstances
super-convergence can be achieved with the CLR method. Using Cifar-10 and
Cifar-100 \cite{Cifar}, ImageNet \cite{ImageNet} and a variety of network
architectures, they were only able to induce super-convergence on Cifar-10 and
Cifar-100 using ResNet, and not at all on ImageNet.

There is still a significant amount of work to be done on investigating
super-convergence. It is still unclear when it can occur and if it can be
induced in a deterministic manner in other circumstances. In this paper we
investigate the current known quantities of super-convergence and attempt to
reproduce them.

\subsection{Target Questions}
We seek to reproduce the key experiments in \cite{SuperConvergence} and confirm
most significant claims made by the authors. Specifically, we aim to:
\begin{itemize}
    \item Confirm the existence of the super-convergence phenomenon
    \item Verify that super-convergence is superior to the PCLR method with
        limited training examples
    \item Verify the reported accuracy trends of using varying step sizes,
        learning rate ranges, and iterations for the CLR method
\end{itemize}
%------------------------------------------------------------------------------

\section{Method}
\label{sec:method}
Due to time and hardware constraints (lack of multiple GPUs), we were unable to
replicate all of the experiments that were listed in the paper. Therefore, we
had to choose which experiments from which we could glean the most. It was our
goal to show the existence or not of the following claims made in the paper.
Super-convergence:
\begin{itemize}
    \item allows the network to achieve higher test accuracy than traditional
        piece-wise constant learning rates (PCLR)
    \item requires an order of magnitude fewer iterations to converge than PCLR
    \item is more noticeable with fewer training examples
    \item works with various ResNet sizes
    \item works best with a certain step size
\end{itemize}

When available, we used the hyperparameters that the authors listed in their
paper. When these were omitted, we attempted to use standard values that are
used on Cifar-10 and ResNets.

\subsection{Implementation Details}
In the ICLR submitted paper, the authors mentioned that they would release
their code upon acceptance to the conference. However, upon Googling the paper
name, we found an arXiv version of the paper that links to the authors' source
code on GitHub. We chose to not look at this code and reimplement their
solution so that we could test the reproducibility under the information
(especially hyperparameters) that were listed in the ICLR paper.

As a starting point, we used the open source ResNet training on Cifar-10 code
available in the TensorFlow \cite{TensorFlow} repository. We modified the code
accordingly for each of the experiments.

A major limitation in our testing was our restriction to a single GPU for all of the experiments. The authors stated that they used 8 Nvidia Titan Black GPUs, which allowed them to test with batch sizes as large as 1536. We were restricted to a batch size of 256. The authors found that increasing the batch size significantly improved the performance of their CLR tests, so the discrepancy between our results and the authors could be due to this.

%------------------------------------------------------------------------------

\section{Experiments}
Keivaun
\subsection{Methodology}
%------------------------------------------------------------------------------

\section{Conclusion}
\label{sec:conclusion}
We confirm the existence of the ``super-convergence'' phenomenon under the
circumstances given by the authors: ResNets can be trained an order of
magnitude more quickly on Cifar 10 and Cifar 100 using large, cyclical learning
rates. Unlike the authors, however, we find that the accuracy achieved using
this method is below that of using a piecewise-constant learning rate. In
addition, we find that the CLR method is not superior to the PCLR method when
limited training data is used, contrary to the authors' experiments. When
adjusting step-size, learning rate range, and number of iterations, we observe
trends similar to those of the authors.

The discrepancy in peak accuracy between our super-convergence experiments and
those of the authors may be attributable to the very large batch size of 1000
used by the authors. This large batch size allows the authors to train their
network more quickly than we could, as we do not possess the hardware to use
such a large batch size. However, the general trends should and do still hold.

We believe that circumstances under which super-convergence occurs are too
limited to be considered useful and surmise that further investigation into the
benefits of large and cyclical learning rates rather than trying to reproduce
may be an isolated phenomenon under different circumstances.

\subsection{Cost of Reproduction}
What cost in terms of resources (computation, time, people, development effort, communication with the authors).

{\small
\bibliographystyle{ieee}
\bibliography{bib}
}

\end{document}
